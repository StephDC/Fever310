# load data
df <- read.csv(data_path)
# features we want
features = c("reanalysis_specific_humidity_g_per_kg", "reanalysis_dew_point_temp_k",
"station_avg_temp_c", "station_min_temp_c")
# fill missing values
df[features] %<>% na.locf(fromLast = TRUE)
# add city if labels data aren't provided
if (is.null(labels_path)) features %<>% c("city", "year", "weekofyear")
# select features we want
df <- df[features]
# add labels to dataframe
if (!is.null(labels_path)) df %<>% cbind(read.csv(labels_path))
# filter by city
df_sj <- filter(df, city == 'sj')
df_iq <- filter(df, city == 'iq')
# return a list with the 2 data frames
return(list(df_sj, df_iq))
}
# preprocess the .csv files
preprocessData(data_path = 'dengue_features_train.csv', labels_path = 'dengue_labels_train.csv') -> trains
## Data Loading
train_features = read.csv('~/DS310 Dengue/Data/dengue_features_train.csv')
train_labels   = read.csv('~/DS310 Dengue/Data/dengue_labels_train.csv')
# Seperate data by city
sj_train_features = train_features %>% filter(city == 'sj')
sj_train_labels   = train_labels   %>% filter(city == 'sj')
iq_train_features = train_features %>% filter(city == 'iq')
iq_train_labels   = train_labels   %>% filter(city == 'iq')
# data shape for each city
cat('\nSan Juan\n',
'\t features: ', sj_train_features %>% ncol,
'\t entries: ' , sj_train_features %>% nrow,
'\t labels: '  , sj_train_labels %>% nrow)
cat('\nIquitos\n',
'\t features: ', iq_train_features %>% ncol,
'\t entries: ' , iq_train_features %>% nrow,
'\t labels: '  , iq_train_labels %>% nrow)
preprocessData <- function(data_path, labels_path = NULL)
{
# load data
df <- read.csv(data_path)
# features we want
features = c("reanalysis_specific_humidity_g_per_kg", "reanalysis_dew_point_temp_k",
"station_avg_temp_c", "station_min_temp_c")
# fill missing values
df[features] %<>% na.locf(fromLast = TRUE)
# add city if labels data aren't provided
if (is.null(labels_path)) features %<>% c("city", "year", "weekofyear")
# select features we want
df <- df[features]
# add labels to dataframe
if (!is.null(labels_path)) df %<>% cbind(read.csv(labels_path))
# filter by city
df_sj <- filter(df, city == 'sj')
df_iq <- filter(df, city == 'iq')
# return a list with the 2 data frames
return(list(df_sj, df_iq))
}
# preprocess the .csv files
preprocessData(data_path = '~/DS310 Dengue/Data/dengue_features_train.csv', labels_path = '~/DS310 Dengue/Data/dengue_labels_train.csv') -> trains
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library('randomForest') # classification algorithm
library('rSymPy')
library('polynom')
# load libraries
pkgs <- c('tidyverse', 'corrplot', 'magrittr', 'zoo', 'RColorBrewer', 'gridExtra','MASS')
invisible(lapply(pkgs, require, character.only = T))
## Data Loading
train_features = read.csv('dengue_features_train.csv')
install.packages(c("tidyverse", "corrplot", "zoo", "gridExtra", "MASS"))
# load libraries
pkgs <- c('tidyverse', 'corrplot', 'magrittr', 'zoo', 'RColorBrewer', 'gridExtra','MASS')
invisible(lapply(pkgs, require, character.only = T))
## Data Loading
train_features = read.csv('dengue_features_train.csv')
# load libraries
pkgs <- c('tidyverse', 'corrplot', 'magrittr', 'zoo', 'RColorBrewer', 'gridExtra','MASS')
invisible(lapply(pkgs, require, character.only = T))
## Data Loading
train_features = read.csv('~/DS310 Dengue/Data/dengue_features_train.csv')
train_labels   = read.csv('~/DS310 Dengue/Data/dengue_labels_train.csv')
# Seperate data by city
sj_train_features = train_features %>% filter(city == 'sj')
sj_train_labels   = train_labels   %>% filter(city == 'sj')
iq_train_features = train_features %>% filter(city == 'iq')
iq_train_labels   = train_labels   %>% filter(city == 'iq')
# data shape for each city
cat('\nSan Juan\n',
'\t features: ', sj_train_features %>% ncol,
'\t entries: ' , sj_train_features %>% nrow,
'\t labels: '  , sj_train_labels %>% nrow)
##
## San Juan
##       features:  24   entries:  936   labels:  936
cat('\nIquitos\n',
'\t features: ', iq_train_features %>% ncol,
'\t entries: ' , iq_train_features %>% nrow,
'\t labels: '  , iq_train_labels %>% nrow)
##
## Iquitos
##       features:  24   entries:  520   labels:  520
# glimspe at the data (first 7 columns only to maintain neat display)
head(sj_train_features[1:7])
##   city year weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se
## 1   sj 1990         18      1990-04-30 0.1226000 0.1037250 0.1984833
## 2   sj 1990         19      1990-05-07 0.1699000 0.1421750 0.1623571
## 3   sj 1990         20      1990-05-14 0.0322500 0.1729667 0.1572000
## 4   sj 1990         21      1990-05-21 0.1286333 0.2450667 0.2275571
## 5   sj 1990         22      1990-05-28 0.1962000 0.2622000 0.2512000
## 6   sj 1990         23      1990-06-04        NA 0.1748500 0.2543143
# Remove `week_start_date` string.
sj_train_features %<>% dplyr::select(-week_start_date)
iq_train_features %<>% dplyr::select(-week_start_date)
# count missing values (as percent)
apply(sj_train_features, 2, function(x)
round(100 * (length(which(is.na(x))))/length(x) , digits = 1)) %>%
as.data.frame() %>%
`names<-`('Percent of Missing Values')
##                                       Percent of Missing Values
## city                                                        0.0
## year                                                        0.0
## weekofyear                                                  0.0
## ndvi_ne                                                    20.4
## ndvi_nw                                                     5.2
## ndvi_se                                                     2.0
## ndvi_sw                                                     2.0
## precipitation_amt_mm                                        1.0
## reanalysis_air_temp_k                                       0.6
## reanalysis_avg_temp_k                                       0.6
## reanalysis_dew_point_temp_k                                 0.6
## reanalysis_max_air_temp_k                                   0.6
## reanalysis_min_air_temp_k                                   0.6
## reanalysis_precip_amt_kg_per_m2                             0.6
## reanalysis_relative_humidity_percent                        0.6
## reanalysis_sat_precip_amt_mm                                1.0
## reanalysis_specific_humidity_g_per_kg                       0.6
## reanalysis_tdtr_k                                           0.6
## station_avg_temp_c                                          0.6
## station_diur_temp_rng_c                                     0.6
## station_max_temp_c                                          0.6
## station_min_temp_c                                          0.6
## station_precip_mm                                           0.6
sj_train_features %>%
mutate(index = as.numeric(row.names(.))) %>%
ggplot(aes(index, ndvi_ne)) +
geom_line(colour = 'dodgerblue') +
ggtitle("Vegetation Index over Time")
# impute NAs by the latest value
sj_train_features$ndvi_ne %<>% na.locf(fromLast = TRUE)
iq_train_features$ndvi_ne %<>% na.locf(fromLast = TRUE)
# distibution of labels
cat('\nSan Juan\n',
'\t total cases mean: ',      sj_train_labels$total_cases %>% mean(),
'\t total cases variance: ' , sj_train_labels$total_cases %>% var() )
##
## San Juan
##       total cases mean:  34.18056     total cases variance:  2640.045
cat('\nIquitos\n',
'\t total cases mean: ',      iq_train_labels$total_cases %>% mean(),
'\t total cases variance: ' , iq_train_labels$total_cases %>% var() )
##
## Iquitos
##       total cases mean:  7.565385     total cases variance:  115.8955
# total cases of dengue: histograms
rbind(iq_train_labels, sj_train_labels) %>%
ggplot(aes(x = total_cases,fill = ..count..)) +
geom_histogram(bins = 12, colour = 'black') + ggtitle('Total Cases of Dengue') +
scale_y_continuous(breaks = seq(0,700,100)) + facet_wrap(~city)
# corerlations between features
sj_train_features %<>% mutate('total_cases' = sj_train_labels$total_cases)
iq_train_features %<>% mutate('total_cases' = iq_train_labels$total_cases)
# plot san juan correlation matrix
sj_train_features %>%
dplyr::select(-city, -year, -weekofyear) %>%
cor(use = 'pairwise.complete.obs') -> M1
corrplot(M1, type="lower", method="color",
col=brewer.pal(n=8, name="RdBu"),diag=FALSE)
# plot iquitos correlation matrix
iq_train_features %>%
dplyr::select(-city, -year, -weekofyear) %>%
cor(use = 'pairwise.complete.obs') -> M2
corrplot(M2, type="lower", method="color",
col=brewer.pal(n=8, name="RdBu"),diag=FALSE)
# see the correlations as barplot
sort(M1[21,-21]) %>%
as.data.frame %>%
`names<-`('correlation') %>%
ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) +
geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits =  c(-.15,.25)) +
labs(title = 'San Jose\n Correlations', x = NULL, y = NULL) + coord_flip() -> cor1
# can use ncol(M1) instead of 21 to generalize the code
sort(M2[21,-21]) %>%
as.data.frame %>%
`names<-`('correlation') %>%
ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) +
geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits =  c(-.15,.25)) +
labs(title = 'Iquitos\n Correlations', x = NULL, y = NULL) + coord_flip() -> cor2
grid.arrange(cor1, cor2, nrow = 1)
preprocessData <- function(data_path, labels_path = NULL)
{
# load data
df <- read.csv(data_path)
# features we want
features = c("reanalysis_specific_humidity_g_per_kg", "reanalysis_dew_point_temp_k",
"station_avg_temp_c", "station_min_temp_c")
# fill missing values
df[features] %<>% na.locf(fromLast = TRUE)
# add city if labels data aren't provided
if (is.null(labels_path)) features %<>% c("city", "year", "weekofyear")
# select features we want
df <- df[features]
# add labels to dataframe
if (!is.null(labels_path)) df %<>% cbind(read.csv(labels_path))
# filter by city
df_sj <- filter(df, city == 'sj')
df_iq <- filter(df, city == 'iq')
# return a list with the 2 data frames
return(list(df_sj, df_iq))
}
# preprocess the .csv files
preprocessData(data_path = 'dengue_features_train.csv', labels_path = 'dengue_labels_train.csv') -> trains
# load libraries
pkgs <- c('tidyverse', 'corrplot', 'magrittr', 'zoo', 'RColorBrewer', 'gridExtra','MASS')
invisible(lapply(pkgs, require, character.only = T))
## Data Loading
train_features = read.csv('~/DS310 Dengue/Data/dengue_features_train.csv')
train_labels   = read.csv('~/DS310 Dengue/Data/dengue_labels_train.csv')
# Seperate data by city
sj_train_features = train_features %>% filter(city == 'sj')
sj_train_labels   = train_labels   %>% filter(city == 'sj')
iq_train_features = train_features %>% filter(city == 'iq')
iq_train_labels   = train_labels   %>% filter(city == 'iq')
# data shape for each city
cat('\nSan Juan\n',
'\t features: ', sj_train_features %>% ncol,
'\t entries: ' , sj_train_features %>% nrow,
'\t labels: '  , sj_train_labels %>% nrow)
##
## San Juan
##       features:  24   entries:  936   labels:  936
cat('\nIquitos\n',
'\t features: ', iq_train_features %>% ncol,
'\t entries: ' , iq_train_features %>% nrow,
'\t labels: '  , iq_train_labels %>% nrow)
##
## Iquitos
##       features:  24   entries:  520   labels:  520
# glimspe at the data (first 7 columns only to maintain neat display)
head(sj_train_features[1:7])
##   city year weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se
## 1   sj 1990         18      1990-04-30 0.1226000 0.1037250 0.1984833
## 2   sj 1990         19      1990-05-07 0.1699000 0.1421750 0.1623571
## 3   sj 1990         20      1990-05-14 0.0322500 0.1729667 0.1572000
## 4   sj 1990         21      1990-05-21 0.1286333 0.2450667 0.2275571
## 5   sj 1990         22      1990-05-28 0.1962000 0.2622000 0.2512000
## 6   sj 1990         23      1990-06-04        NA 0.1748500 0.2543143
# Remove `week_start_date` string.
sj_train_features %<>% dplyr::select(-week_start_date)
iq_train_features %<>% dplyr::select(-week_start_date)
# count missing values (as percent)
apply(sj_train_features, 2, function(x)
round(100 * (length(which(is.na(x))))/length(x) , digits = 1)) %>%
as.data.frame() %>%
`names<-`('Percent of Missing Values')
##                                       Percent of Missing Values
## city                                                        0.0
## year                                                        0.0
## weekofyear                                                  0.0
## ndvi_ne                                                    20.4
## ndvi_nw                                                     5.2
## ndvi_se                                                     2.0
## ndvi_sw                                                     2.0
## precipitation_amt_mm                                        1.0
## reanalysis_air_temp_k                                       0.6
## reanalysis_avg_temp_k                                       0.6
## reanalysis_dew_point_temp_k                                 0.6
## reanalysis_max_air_temp_k                                   0.6
## reanalysis_min_air_temp_k                                   0.6
## reanalysis_precip_amt_kg_per_m2                             0.6
## reanalysis_relative_humidity_percent                        0.6
## reanalysis_sat_precip_amt_mm                                1.0
## reanalysis_specific_humidity_g_per_kg                       0.6
## reanalysis_tdtr_k                                           0.6
## station_avg_temp_c                                          0.6
## station_diur_temp_rng_c                                     0.6
## station_max_temp_c                                          0.6
## station_min_temp_c                                          0.6
## station_precip_mm                                           0.6
sj_train_features %>%
mutate(index = as.numeric(row.names(.))) %>%
ggplot(aes(index, ndvi_ne)) +
geom_line(colour = 'dodgerblue') +
ggtitle("Vegetation Index over Time")
# impute NAs by the latest value
sj_train_features$ndvi_ne %<>% na.locf(fromLast = TRUE)
iq_train_features$ndvi_ne %<>% na.locf(fromLast = TRUE)
# distibution of labels
cat('\nSan Juan\n',
'\t total cases mean: ',      sj_train_labels$total_cases %>% mean(),
'\t total cases variance: ' , sj_train_labels$total_cases %>% var() )
##
## San Juan
##       total cases mean:  34.18056     total cases variance:  2640.045
cat('\nIquitos\n',
'\t total cases mean: ',      iq_train_labels$total_cases %>% mean(),
'\t total cases variance: ' , iq_train_labels$total_cases %>% var() )
##
## Iquitos
##       total cases mean:  7.565385     total cases variance:  115.8955
# total cases of dengue: histograms
rbind(iq_train_labels, sj_train_labels) %>%
ggplot(aes(x = total_cases,fill = ..count..)) +
geom_histogram(bins = 12, colour = 'black') + ggtitle('Total Cases of Dengue') +
scale_y_continuous(breaks = seq(0,700,100)) + facet_wrap(~city)
# corerlations between features
sj_train_features %<>% mutate('total_cases' = sj_train_labels$total_cases)
iq_train_features %<>% mutate('total_cases' = iq_train_labels$total_cases)
# plot san juan correlation matrix
sj_train_features %>%
dplyr::select(-city, -year, -weekofyear) %>%
cor(use = 'pairwise.complete.obs') -> M1
corrplot(M1, type="lower", method="color",
col=brewer.pal(n=8, name="RdBu"),diag=FALSE)
# plot iquitos correlation matrix
iq_train_features %>%
dplyr::select(-city, -year, -weekofyear) %>%
cor(use = 'pairwise.complete.obs') -> M2
corrplot(M2, type="lower", method="color",
col=brewer.pal(n=8, name="RdBu"),diag=FALSE)
# see the correlations as barplot
sort(M1[21,-21]) %>%
as.data.frame %>%
`names<-`('correlation') %>%
ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) +
geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits =  c(-.15,.25)) +
labs(title = 'San Jose\n Correlations', x = NULL, y = NULL) + coord_flip() -> cor1
# can use ncol(M1) instead of 21 to generalize the code
sort(M2[21,-21]) %>%
as.data.frame %>%
`names<-`('correlation') %>%
ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) +
geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits =  c(-.15,.25)) +
labs(title = 'Iquitos\n Correlations', x = NULL, y = NULL) + coord_flip() -> cor2
grid.arrange(cor1, cor2, nrow = 1)
preprocessData <- function(data_path, labels_path = NULL)
{
# load data
df <- read.csv(data_path)
# features we want
features = c("reanalysis_specific_humidity_g_per_kg", "reanalysis_dew_point_temp_k",
"station_avg_temp_c", "station_min_temp_c")
# fill missing values
df[features] %<>% na.locf(fromLast = TRUE)
# add city if labels data aren't provided
if (is.null(labels_path)) features %<>% c("city", "year", "weekofyear")
# select features we want
df <- df[features]
# add labels to dataframe
if (!is.null(labels_path)) df %<>% cbind(read.csv(labels_path))
# filter by city
df_sj <- filter(df, city == 'sj')
df_iq <- filter(df, city == 'iq')
# return a list with the 2 data frames
return(list(df_sj, df_iq))
}
# preprocess the .csv files
preprocessData(data_path = '~/DS310 Dengue/Data/dengue_features_train.csv', labels_path = '~/DS310 Dengue/Data/dengue_labels_train.csv') -> trains
sj_train <- trains[[1]]; iq_train <- as.data.frame(trains[2])
summary(sj_train)
##  reanalysis_specific_humidity_g_per_kg reanalysis_dew_point_temp_k
##  Min.   :11.72                         Min.   :289.6
##  1st Qu.:15.23                         1st Qu.:293.8
##  Median :16.83                         Median :295.4
##  Mean   :16.54                         Mean   :295.1
##  3rd Qu.:17.85                         3rd Qu.:296.4
##  Max.   :19.44                         Max.   :297.8
##  station_avg_temp_c station_min_temp_c city          year
##  Min.   :22.84      Min.   :17.80      iq:  0   Min.   :1990
##  1st Qu.:25.81      1st Qu.:21.70      sj:936   1st Qu.:1994
##  Median :27.21      Median :22.80               Median :1999
##  Mean   :27.00      Mean   :22.59               Mean   :1999
##  3rd Qu.:28.18      3rd Qu.:23.90               3rd Qu.:2003
##  Max.   :30.07      Max.   :25.60               Max.   :2008
##    weekofyear     total_cases
##  Min.   : 1.00   Min.   :  0.00
##  1st Qu.:13.75   1st Qu.:  9.00
##  Median :26.50   Median : 19.00
##  Mean   :26.50   Mean   : 34.18
##  3rd Qu.:39.25   3rd Qu.: 37.00
##  Max.   :53.00   Max.   :461.00
summary(iq_train)
##  reanalysis_specific_humidity_g_per_kg reanalysis_dew_point_temp_k
##  Min.   :12.11                         Min.   :290.1
##  1st Qu.:16.12                         1st Qu.:294.6
##  Median :17.43                         Median :295.9
##  Mean   :17.10                         Mean   :295.5
##  3rd Qu.:18.19                         3rd Qu.:296.6
##  Max.   :20.46                         Max.   :298.4
##  station_avg_temp_c station_min_temp_c city          year
##  Min.   :21.40      Min.   :14.70      iq:520   Min.   :2000
##  1st Qu.:27.00      1st Qu.:20.60      sj:  0   1st Qu.:2003
##  Median :27.57      Median :21.35               Median :2005
##  Mean   :27.52      Mean   :21.20               Mean   :2005
##  3rd Qu.:28.09      3rd Qu.:22.00               3rd Qu.:2007
##  Max.   :30.80      Max.   :24.20               Max.   :2010
##    weekofyear     total_cases
##  Min.   : 1.00   Min.   :  0.000
##  1st Qu.:13.75   1st Qu.:  1.000
##  Median :26.50   Median :  5.000
##  Mean   :26.50   Mean   :  7.565
##  3rd Qu.:39.25   3rd Qu.:  9.000
##  Max.   :53.00   Max.   :116.000
# split up the data
sj_train_subtrain <- head(sj_train, 800)
sj_train_subtest  <- tail(sj_train, nrow(sj_train) - 800)
iq_train_subtrain <- head(iq_train, 400)
iq_train_subtest  <- tail(iq_train, nrow(sj_train) - 400)
# function that returns Mean Absolute Error
mae <- function(error) return(mean(abs(error)) )
get_bst_model <- function(train, test)
{
# Step 1: specify the form of the model
form <- "total_cases ~ 1 +
reanalysis_specific_humidity_g_per_kg +
reanalysis_dew_point_temp_k +
station_avg_temp_c +
station_min_temp_c"
grid = 10 ^(seq(-8, -3,1))
best_alpha = c()
best_score = 1000
# Step 2: Find the best hyper parameter, alpha
for (i in grid)
{
model = glm.nb(formula = form,
data = train,
init.theta = i)
results <-  predict(model, test)
score   <-  mae(test$total_cases - results)
if (score < best_score) {
best_alpha <- i
best_score <- score
cat('\nbest score = ', best_score, '\twith alpha = ', best_alpha)
}
}
# Step 3: refit on entire dataset
combined <- rbind(train, test)
combined_model = glm.nb(formula=form,
data = combined,
init.theta = best_alpha)
return (combined_model)
}
sj_model <- get_bst_model(sj_train_subtrain, sj_train_subtest)
##
## best score =  21.01167   with alpha =  1e-08
iq_model <- get_bst_model(iq_train_subtrain, iq_train_subtest)
##
## best score =  6.421811   with alpha =  1e-08
## best score =  6.421811   with alpha =  1e-07
## best score =  6.421811   with alpha =  1e-06
## best score =  6.421811   with alpha =  1e-05
# plot sj
sj_train$fitted = predict(sj_model, sj_train, type = 'response')
sj_train %>%
mutate(index = as.numeric(row.names(.))) %>%
ggplot(aes(x = index)) + ggtitle("San Jose") +
geom_line(aes(y = total_cases, colour = "total_cases")) +
geom_line(aes(y = fitted, colour = "fitted"))
# plot iq
iq_train$fitted = predict(iq_model, iq_train, type = 'response')
iq_train %>%
mutate(index = as.numeric(row.names(.))) %>%
ggplot(aes(x = index)) + ggtitle("Iquitos") +
geom_line(aes(y = total_cases, colour = "total_cases")) +
geom_line(aes(y = fitted, colour = "fitted"))
# submitting the predictions
tests <- preprocessData('~/DS310 Dengue/Data/dengue_features_test.csv')
sj_test <- tests[[1]]; iq_test <- tests[[2]]
sj_test$predicted = predict(sj_model , sj_test, type = 'response')
iq_test$predicted = predict(iq_model , iq_test, type = 'response')
submissions = read.csv('submission_format.csv')
# submitting the predictions
tests <- preprocessData('~/DS310 Dengue/Data/dengue_features_test.csv')
sj_test <- tests[[1]]; iq_test <- tests[[2]]
sj_test$predicted = predict(sj_model , sj_test, type = 'response')
iq_test$predicted = predict(iq_model , iq_test, type = 'response')
submissions = read.csv('~/DS310 Dengue/Data/submission_format.csv')
inner_join(submissions, rbind(sj_test,iq_test)) %>%
dplyr::select(city, year, weekofyear, total_cases = predicted) ->
predictions
## Joining, by = c("city", "year", "weekofyear")
predictions$total_cases %<>% round()
write.csv(predictions, 'submissions/predictions.csv', row.names = FALSE)
# submitting the predictions
tests <- preprocessData('~/DS310 Dengue/Data/dengue_features_test.csv')
sj_test <- tests[[1]]; iq_test <- tests[[2]]
sj_test$predicted = predict(sj_model , sj_test, type = 'response')
iq_test$predicted = predict(iq_model , iq_test, type = 'response')
submissions = read.csv('~/DS310 Dengue/Data/submission_format.csv')
inner_join(submissions, rbind(sj_test,iq_test)) %>%
dplyr::select(city, year, weekofyear, total_cases = predicted) ->
predictions
## Joining, by = c("city", "year", "weekofyear")
predictions$total_cases %<>% round()
write.csv(predictions, 'submitpredictions.csv', row.names = FALSE)
write.csv(predictions, 'submitpredictions.csv', row.names = FALSE)
predictions$total_cases %<>% round()
write.csv(predictions, 'submitpredictions.csv', row.names = FALSE)
